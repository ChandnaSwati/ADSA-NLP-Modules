{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4d4482",
   "metadata": {},
   "source": [
    "\n",
    "# F1 Score & Cross-Entropy — Mini Exercises\n",
    "\n",
    "This notebook contains **two exercises per metric** (F1 and Cross-Entropy) with space for you to compute the answers, **plus full solutions** further below.\n",
    "\n",
    "**How to use:**\n",
    "1. Work through each exercise cell under *Your Turn*.\n",
    "2. Check your work against the **Solution** cells that follow.\n",
    "3. All logarithms use the natural log (`ln`).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ba651",
   "metadata": {},
   "source": [
    "\n",
    "## Part A — F1 Score\n",
    "\n",
    "### Exercise A1 (Binary)\n",
    "You are given **true** and **predicted** labels for 8 samples:\n",
    "\n",
    "| Sample | True | Pred |\n",
    "|:--:|:--:|:--:|\n",
    "| 1 | 1 | 1 |\n",
    "| 2 | 0 | 1 |\n",
    "| 3 | 1 | 1 |\n",
    "| 4 | 0 | 0 |\n",
    "| 5 | 1 | 0 |\n",
    "| 6 | 0 | 0 |\n",
    "| 7 | 1 | 1 |\n",
    "| 8 | 0 | 0 |\n",
    "\n",
    "**Tasks:**  \n",
    "1) Build the confusion matrix (TP, FP, FN, TN).  \n",
    "2) Compute **Precision**, **Recall**, and **F1**.  \n",
    "*(F1 = 2 · (P · R) / (P + R))*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06954d47",
   "metadata": {},
   "source": [
    "#### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace nothing; use as a workspace. Data is preloaded.\n",
    "y_true = [1,0,1,0,1,0,1,0]\n",
    "y_pred = [1,1,1,0,0,0,1,0]\n",
    "\n",
    "# TODO: compute TP, FP, FN, TN, then Precision, Recall, F1\n",
    "# Hints:\n",
    "# TP = sum(yt==1 and yp==1), FP = sum(yt==0 and yp==1)\n",
    "# FN = sum(yt==1 and yp==0), TN = sum(yt==0 and yp==0)\n",
    "\n",
    "TP = sum(1 for yt, yp in zip(y_true, y_pred) if yt==1 and yp==1)\n",
    "FP = sum(1 for yt, yp in zip(y_true, y_pred) if yt==0 and yp==1)\n",
    "FN = sum(1 for yt, yp in zip(y_true, y_pred) if yt==1 and yp==0)\n",
    "TN = sum(1 for yt, yp in zip(y_true, y_pred) if yt==0 and yp==0)\n",
    "\n",
    "precision = TP / (TP + FP) if (TP+FP)>0 else 0.0\n",
    "recall = TP / (TP + FN) if (TP+FN)>0 else 0.0\n",
    "f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0.0\n",
    "\n",
    "TP, FP, FN, TN, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728cc0f",
   "metadata": {},
   "source": [
    "#### Solution (A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e884862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expected: TP=3, FP=1, FN=1, TN=3; Precision=0.75, Recall=0.75, F1=0.75\n",
    "expected = {\n",
    "    \"TP\": 3, \"FP\": 1, \"FN\": 1, \"TN\": 3,\n",
    "    \"precision\": 0.75, \"recall\": 0.75, \"f1\": 0.75\n",
    "}\n",
    "expected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb8f88",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise A2 (Multiclass, Macro-F1)\n",
    "Three classes: **A, B, C**. For 9 samples, the model predicted:\n",
    "\n",
    "| Sample | True | Pred |\n",
    "|:--:|:--:|:--:|\n",
    "| 1 | A | A |\n",
    "| 2 | A | B |\n",
    "| 3 | A | C |\n",
    "| 4 | B | B |\n",
    "| 5 | B | B |\n",
    "| 6 | B | A |\n",
    "| 7 | C | C |\n",
    "| 8 | C | B |\n",
    "| 9 | C | C |\n",
    "\n",
    "**Tasks:**  \n",
    "1) For each class, compute **Precision**, **Recall**, **F1** (treat that class as “positive,” others as “negative”).  \n",
    "2) Compute **Macro-F1** (average of per-class F1’s).  \n",
    "*(Optional: also report accuracy and compare to Macro-F1.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4964626",
   "metadata": {},
   "source": [
    "#### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a910004",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true_labels = [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"C\",\"C\",\"C\"]\n",
    "pred_labels = [\"A\",\"B\",\"C\",\"B\",\"B\",\"A\",\"C\",\"B\",\"C\"]\n",
    "classes = sorted(set(true_labels))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def prf1_for_class(cls, y_true, y_pred, labels):\n",
    "    TP = sum(1 for t,p in zip(y_true,y_pred) if t==cls and p==cls)\n",
    "    FP = sum(1 for t,p in zip(y_true,y_pred) if t!=cls and p==cls)\n",
    "    FN = sum(1 for t,p in zip(y_true,y_pred) if t==cls and p!=cls)\n",
    "    precision = TP / (TP + FP) if (TP+FP)>0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP+FN)>0 else 0.0\n",
    "    f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0.0\n",
    "    return {\"TP\":TP,\"FP\":FP,\"FN\":FN,\"precision\":precision,\"recall\":recall,\"f1\":f1}\n",
    "\n",
    "per_class = {c: prf1_for_class(c, true_labels, pred_labels, classes) for c in classes}\n",
    "macro_f1 = sum(per_class[c][\"f1\"] for c in classes) / len(classes)\n",
    "\n",
    "accuracy = sum(t==p for t,p in zip(true_labels, pred_labels)) / len(true_labels)\n",
    "\n",
    "per_class, macro_f1, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6cbb4",
   "metadata": {},
   "source": [
    "#### Solution (A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expected per-class F1 and Macro-F1:\n",
    "expected_A = {\"precision\": 0.5, \"recall\": 1/3, \"f1\": 0.4}\n",
    "expected_B = {\"precision\": 0.5, \"recall\": 2/3, \"f1\": 4/7}   # ≈0.5714\n",
    "expected_C = {\"precision\": 2/3, \"recall\": 2/3, \"f1\": 2/3}   # ≈0.6667\n",
    "macro_f1_expected = (expected_A[\"f1\"] + expected_B[\"f1\"] + expected_C[\"f1\"]) / 3\n",
    "accuracy_expected = 5/9\n",
    "{\n",
    "    \"Class A\": expected_A,\n",
    "    \"Class B\": expected_B,\n",
    "    \"Class C\": expected_C,\n",
    "    \"Macro-F1\": float(macro_f1_expected),\n",
    "    \"Accuracy\": float(accuracy_expected),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1db85",
   "metadata": {},
   "source": [
    "\n",
    "## Part B — Cross-Entropy Loss  \n",
    "Use **natural log (`ln`)**. For one-hot labels, the loss per sample is **−ln(p_true)**.\n",
    "\n",
    "### Exercise B1 (3 Classes)\n",
    "Softmax probabilities are given as **[p(class 0), p(class 1), p(class 2)]**.\n",
    "\n",
    "| Sample | True (one-hot) | Predicted Probabilities |\n",
    "|:--:|:--:|:--:|\n",
    "| 1 | [1, 0, 0] | [0.7, 0.2, 0.1] |\n",
    "| 2 | [0, 1, 0] | [0.1, 0.6, 0.3] |\n",
    "| 3 | [0, 0, 1] | [0.2, 0.2, 0.6] |\n",
    "| 4 | [0, 1, 0] | [0.3, 0.4, 0.3] |\n",
    "\n",
    "**Tasks:**  \n",
    "1) Compute the **loss per sample**: −ln(probability of the true class).  \n",
    "2) Compute the **average loss**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae1f1d",
   "metadata": {},
   "source": [
    "#### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "# One-hot labels as the true class index\n",
    "true_idx = [0,1,2,1]\n",
    "pred_probs = [\n",
    "    [0.7,0.2,0.1],\n",
    "    [0.1,0.6,0.3],\n",
    "    [0.2,0.2,0.6],\n",
    "    [0.3,0.4,0.3],\n",
    "]\n",
    "\n",
    "losses = [-math.log(probs[i]) for i, probs in zip(true_idx, pred_probs)]\n",
    "avg_loss = sum(losses)/len(losses)\n",
    "losses, avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b74afd",
   "metadata": {},
   "source": [
    "#### Solution (B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expected losses and average (natural log):\n",
    "expected_losses = [\n",
    "    -math.log(0.7),  # ≈0.3567\n",
    "    -math.log(0.6),  # ≈0.5108\n",
    "    -math.log(0.6),  # ≈0.5108\n",
    "    -math.log(0.4),  # ≈0.9163\n",
    "]\n",
    "expected_avg = sum(expected_losses)/len(expected_losses)\n",
    "expected_losses, expected_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5fb0b4",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise B2 (Binary)\n",
    "Let **y ∈ {0,1}** and **p = P(y=1)**. The binary cross-entropy is  \n",
    "**L = −[y·ln(p) + (1−y)·ln(1−p)]**.\n",
    "\n",
    "| Sample | y | p |\n",
    "|:--:|:--:|:--:|\n",
    "| 1 | 1 | 0.90 |\n",
    "| 2 | 0 | 0.80 |\n",
    "| 3 | 1 | 0.30 |\n",
    "| 4 | 0 | 0.40 |\n",
    "| 5 | 1 | 0.60 |\n",
    "\n",
    "**Tasks:**  \n",
    "1) Compute the **loss per sample**.  \n",
    "2) Compute the **average loss**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46179c6",
   "metadata": {},
   "source": [
    "#### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84370340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "ys = [1,0,1,0,1]\n",
    "ps = [0.90, 0.80, 0.30, 0.40, 0.60]\n",
    "\n",
    "def bce(y, p):\n",
    "    # add tiny epsilon for numerical stability (not strictly needed here)\n",
    "    eps = 1e-15\n",
    "    p = min(max(p, eps), 1-eps)\n",
    "    return -(y*math.log(p) + (1-y)*math.log(1-p))\n",
    "\n",
    "losses = [bce(y,p) for y,p in zip(ys, ps)]\n",
    "avg_loss = sum(losses)/len(losses)\n",
    "losses, avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3023c00",
   "metadata": {},
   "source": [
    "#### Solution (B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f066f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expected individual losses and average (natural log):\n",
    "expected_losses = [\n",
    "    -math.log(0.90),  # ≈0.1054\n",
    "    -math.log(0.20),  # ≈1.6094\n",
    "    -math.log(0.30),  # ≈1.2040\n",
    "    -math.log(0.60),  # ≈0.5108  (since y=0, -ln(1-0.40) = -ln(0.60))\n",
    "    -math.log(0.60),  # ≈0.5108\n",
    "]\n",
    "expected_avg = sum(expected_losses)/len(expected_losses)\n",
    "expected_losses, expected_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f61efe0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "- Use **Macro-F1** for class-imbalance-insensitive evaluation in multiclass problems.\n",
    "- Cross-Entropy penalizes **confident mistakes** heavily (e.g., predicting 0.99 for the wrong class).\n",
    "- All computations use natural logarithm (`math.log`), which is `ln`.\n",
    "\n",
    "Happy learning!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
