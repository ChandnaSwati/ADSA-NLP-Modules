{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30160cd2",
   "metadata": {},
   "source": [
    "\n",
    "# ============================================\n",
    "# LangGraph Mini-Lab 4 — Chatbot Agent with Llama (TinyLlama, no API keys)\n",
    "# ============================================\n",
    "**Author:** Dr. Dasha Trofimova\n",
    "\n",
    "This notebook builds a **chatbot-style agent** using **LangGraph** for control flow and an **open-source LLM** (TinyLlama) via `transformers` — mirroring the structure of `Agent_Bot.py`, but **no OpenAI keys** required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331cfcb",
   "metadata": {},
   "source": [
    "\n",
    "### What you'll learn\n",
    "- Wrap a Hugging Face chat model as a simple LLM function\n",
    "- Maintain **conversation state** (messages) inside a LangGraph `State`\n",
    "- Add a **system prompt**, keep the **last N turns**, and run a REPL chat loop\n",
    "- (Optional) add a tiny **calculator tool** and route via a decision node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfba727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q langgraph transformers accelerate sentencepiece --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ff97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "print(\"cuda available?\", torch.cuda.is_available())\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "gen = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    device_map=\"auto\" if DEVICE != -1 else None,\n",
    "    torch_dtype=\"auto\",\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"Loaded:\", model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful teaching assistant. Keep answers concise, concrete, and cite equations or steps when relevant. \"\n",
    "    \"If you don't know, say so briefly.\"\n",
    ")\n",
    "\n",
    "def format_prompt(messages, system=SYSTEM_PROMPT, max_rounds=6):\n",
    "    sys = f\"System: {system}\\n\\n\"\n",
    "    ua = [m for m in messages if m[\"role\"] in (\"user\",\"assistant\")]\n",
    "    ua = ua[-(2*max_rounds):]\n",
    "    lines = []\n",
    "    for m in ua:\n",
    "        role = \"User\" if m[\"role\"] == \"user\" else \"Assistant\"\n",
    "        lines.append(f\"{role}: {m['content']}\")\n",
    "    return sys + \"\\n\".join(lines) + \"\\nAssistant:\"\n",
    "\n",
    "def llm_chat(messages):\n",
    "    prompt = format_prompt(messages)\n",
    "    out = gen(prompt)[0][\"generated_text\"]\n",
    "    if \"Assistant:\" in out:\n",
    "        reply = out.split(\"Assistant:\", 1)[-1].strip()\n",
    "    else:\n",
    "        reply = out.strip()\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2570f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: List[Dict[str,str]]\n",
    "    last_reply: str\n",
    "\n",
    "NODE_DESC = {\n",
    "    \"chat_turn\": \"Calls the TinyLlama model with the current chat history and appends assistant reply.\",\n",
    "}\n",
    "\n",
    "def trace(name):\n",
    "    def deco(fn):\n",
    "        def wrap(state: ChatState) -> ChatState:\n",
    "            print(f\"\\n▶ Node: {name} — {NODE_DESC.get(name,'')}\")\n",
    "            out = fn(state)\n",
    "            return out\n",
    "        wrap.__doc__ = NODE_DESC.get(name,\"\")\n",
    "        return wrap\n",
    "    return deco\n",
    "\n",
    "@trace(\"chat_turn\")\n",
    "def chat_turn(state: ChatState) -> ChatState:\n",
    "    reply = llm_chat(state[\"messages\"])\n",
    "    new_msgs = state[\"messages\"] + [{\"role\":\"assistant\",\"content\":reply}]\n",
    "    return {\"messages\": new_msgs, \"last_reply\": reply}\n",
    "\n",
    "g = StateGraph(ChatState)\n",
    "g.add_node(\"chat_turn\", chat_turn)\n",
    "g.add_edge(\"chat_turn\", END)\n",
    "g.set_entry_point(\"chat_turn\")\n",
    "chatbot = g.compile()\n",
    "print(\"Chatbot agent ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_once(chatbot, messages, user_text):\n",
    "    state = {\"messages\": messages + [{\"role\":\"user\",\"content\":user_text}], \"last_reply\": \"\"}\n",
    "    out = chatbot.invoke(state)\n",
    "    return out[\"messages\"], out[\"last_reply\"]\n",
    "\n",
    "messages = [{\"role\":\"system\",\"content\": SYSTEM_PROMPT}]\n",
    "messages, reply = ask_once(chatbot, messages, \"Hi! What is attention in transformers in one sentence?\")\n",
    "print(\"Assistant:\", reply)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dddb91",
   "metadata": {},
   "source": [
    "\n",
    "## Optional: add a calculator tool via routing\n",
    "We add a **decide** node that routes to either **calc** or **chat_turn**.\n",
    "- Queries with numbers/operators go to the calculator\n",
    "- Everything else goes to the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95625a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from typing import Literal, TypedDict, List, Dict\n",
    "\n",
    "def is_math_query(q: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"[0-9\\s+\\-*/().]+\", q))\n",
    "\n",
    "def calc_eval(q: str) -> str:\n",
    "    try:\n",
    "        return str(eval(q, {\"__builtins__\": {}}, {}))\n",
    "    except Exception as e:\n",
    "        return f\"Calc error: {e}\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[Dict[str,str]]\n",
    "    route: Literal[\"calc\",\"chat\"]\n",
    "    last_reply: str\n",
    "\n",
    "NODE_DESC2 = {\n",
    "    \"decide\": \"Router: if query looks like math, route to 'calc' else 'chat'.\",\n",
    "    \"calc\": \"Calculator: returns the numeric result.\",\n",
    "    \"chat\": \"LLM chat node (TinyLlama).\"\n",
    "}\n",
    "\n",
    "def trace2(name):\n",
    "    def deco(fn):\n",
    "        def wrap(state: AgentState) -> AgentState:\n",
    "            print(f\"\\n▶ Node: {name} — {NODE_DESC2.get(name,'')}\")\n",
    "            out = fn(state)\n",
    "            return out\n",
    "        wrap.__doc__ = NODE_DESC2.get(name,\"\")\n",
    "        return wrap\n",
    "    return deco\n",
    "\n",
    "@trace2(\"decide\")\n",
    "def decide(state: AgentState) -> AgentState:\n",
    "    user_msgs = [m for m in state[\"messages\"] if m[\"role\"] == \"user\"]\n",
    "    last_q = user_msgs[-1][\"content\"] if user_msgs else \"\"\n",
    "    route = \"calc\" if is_math_query(last_q) else \"chat\"\n",
    "    return {\"messages\": state[\"messages\"], \"route\": route, \"last_reply\": state[\"last_reply\"]}\n",
    "\n",
    "@trace2(\"calc\")\n",
    "def calc_node(state: AgentState) -> AgentState:\n",
    "    user_msgs = [m for m in state[\"messages\"] if m[\"role\"] == \"user\"]\n",
    "    last_q = user_msgs[-1][\"content\"] if user_msgs else \"\"\n",
    "    ans = calc_eval(last_q)\n",
    "    msgs = state[\"messages\"] + [{\"role\":\"assistant\",\"content\": ans}]\n",
    "    return {\"messages\": msgs, \"route\": state[\"route\"], \"last_reply\": ans}\n",
    "\n",
    "@trace2(\"chat\")\n",
    "def chat_node(state: AgentState) -> AgentState:\n",
    "    reply = llm_chat(state[\"messages\"])\n",
    "    msgs = state[\"messages\"] + [{\"role\":\"assistant\",\"content\": reply}]\n",
    "    return {\"messages\": msgs, \"route\": state[\"route\"], \"last_reply\": reply}\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "rg = StateGraph(AgentState)\n",
    "rg.add_node(\"decide\", decide)\n",
    "rg.add_node(\"calc\", calc_node)\n",
    "rg.add_node(\"chat\", chat_node)\n",
    "\n",
    "def router(state: AgentState):\n",
    "    return state[\"route\"]\n",
    "\n",
    "rg.add_conditional_edges(\"decide\", router, {\"calc\":\"calc\", \"chat\":\"chat\"})\n",
    "rg.add_edge(\"calc\", END); rg.add_edge(\"chat\", END)\n",
    "rg.set_entry_point(\"decide\")\n",
    "agent = rg.compile()\n",
    "\n",
    "msgs = [{\"role\":\"system\",\"content\": SYSTEM_PROMPT}]\n",
    "def run_agent(msgs, user_text):\n",
    "    state = {\"messages\": msgs + [{\"role\":\"user\",\"content\": user_text}], \"route\": \"chat\", \"last_reply\": \"\"}\n",
    "    out = agent.invoke(state)\n",
    "    return out[\"messages\"], out[\"last_reply\"]\n",
    "\n",
    "msgs, ans1 = run_agent(msgs, \"2*(3+4)\")\n",
    "print(\"Assistant (calc):\", ans1)\n",
    "msgs, ans2 = run_agent(msgs, \"Explain attention in 2 bullets.\")\n",
    "print(\"Assistant (chat):\", ans2[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf36fb4",
   "metadata": {},
   "source": [
    "\n",
    "## Agent Anatomy & Graph\n",
    "- **State keys**: `messages`, `route`, `last_reply`\n",
    "- **Nodes**: `decide` → `calc`/`chat` → END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!apt-get -qq update && apt-get -qq install -y graphviz > /dev/null\n",
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment=\"Chat Agent\", format=\"png\")\n",
    "dot.attr(rankdir=\"LR\", bgcolor=\"white\")\n",
    "dot.node(\"decide\",\"decide()\", shape=\"diamond\", style=\"rounded,filled\", fillcolor=\"#F3F4F6\")\n",
    "dot.node(\"calc\",\"calc()\", shape=\"box\", style=\"rounded,filled\", fillcolor=\"#DBEAFE\")\n",
    "dot.node(\"chat\",\"chat()\", shape=\"box\", style=\"rounded,filled\", fillcolor=\"#DCFCE7\")\n",
    "dot.edge(\"decide\",\"calc\", label=\"looks like math\")\n",
    "dot.edge(\"decide\",\"chat\", label=\"otherwise\")\n",
    "display(dot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785c5a64",
   "metadata": {},
   "source": [
    "\n",
    "## Interactive REPL\n",
    "Run this to chat in the cell. Type `exit` to stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def repl():\n",
    "    msgs = [{\"role\":\"system\",\"content\": SYSTEM_PROMPT}]\n",
    "    while True:\n",
    "        try:\n",
    "            q = input(\"\\nYou: \")\n",
    "        except EOFError:\n",
    "            break\n",
    "        if q.strip().lower() in {\"exit\",\"quit\"}:\n",
    "            break\n",
    "        msgs, ans = run_agent(msgs, q)\n",
    "        print(\"Assistant:\", ans[:1200])\n",
    "    print(\"Bye!\")\n",
    "\n",
    "# Uncomment to use:\n",
    "# repl()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
