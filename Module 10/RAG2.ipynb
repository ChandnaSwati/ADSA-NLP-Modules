{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Imports\n",
    "# =========================================================\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Documents into Chunks\n",
    "\n",
    "1. When we read an entire PDF, we often get very large blocks of text — sometimes thousands of words per page. But LLMs (and embedding models) have context limits — they can’t process all that at once.\n",
    "\n",
    "2. That’s why we split long documents into smaller, overlapping pieces, called chunks.\n",
    "\n",
    "3. Each chunk will later become:\n",
    "   - an embedding (vector representation)\n",
    "   - a retrieval unit (the part that gets fetched when the user asks a question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 598 pages\n",
      "Created 1134 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(\"/Users/swatichandna/SynologyDrive/GitHub/NLP/Module 10/ToyData/test.pdf\")\n",
    "raw_docs = loader.load()\n",
    "print(f\"Loaded {len(raw_docs)} pages\")\n",
    "\n",
    "\n",
    "\n",
    "# Chunk documents\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=120,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "print(f\"Created {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings for Each Chunk\n",
    "\n",
    "Each chunk of text  previous step must be converted into a vector — a list of numbers that captures its meaning.\n",
    "This process is called embedding.\n",
    "\n",
    "Why?\n",
    "1. Machines can’t understand text, but they can compare numbers.\n",
    "2. Each vector encodes the semantic meaning of a chunk.\n",
    "3. Similar meanings → vectors are close together in multi-dimensional space.\n",
    "4. So when a user asks a question, the query is also embedded, and we find the chunks whose vectors are most similar to it (via cosine similarity or L2 distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS retriever ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create embeddings + FAISS retriever\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"FAISS retriever ready\")\n",
    "\n",
    "\n",
    "\n",
    "# Helper to extract clean text\n",
    "def get_context_text(query):\n",
    "    results = retriever.invoke(query)\n",
    "    if not results:\n",
    "        return \"No relevant context found.\"\n",
    "    if hasattr(results[0], \"page_content\"):\n",
    "        texts = [doc.page_content for doc in results]\n",
    "    else:\n",
    "        texts = [str(doc) for doc in results]\n",
    "    return \"\\n\\n\".join(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 20.80it/s]\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hugging Face LLM setup\n",
    "model_id = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n",
    "\n",
    "hf_gen = pipeline(\n",
    "    task=\"text-generation\", #goal is to predict or generate text continuations\n",
    "    model=model,  #Passes in your already loaded language model\n",
    "    tokenizer=tokenizer,  #The tokenizer converts between text and token IDs\n",
    "    max_new_tokens=200, #maximum number of tokens the model is allowed to generate\n",
    "    temperature=0.3, #Controls the randomness of predictions,Lower = more deterministic, Higher = more creative/random.\n",
    "    do_sample=False, #sample probabilistically or pick the top-scoring tokens deterministically.False means greedy decoding — always choose the most likely next token (no randomness).  \n",
    "    top_p=0.9,  #nucleus sampling (a method to pick from only the most probable subset of tokens that together make up 90% of the probability mass).\n",
    "    top_k=50, #Used for top-k sampling, which restricts the model to choose from the top 50 most likely next tokens.\n",
    "    repetition_penalty=1.1, #Penalizes repeating tokens by slightly reducing their probability each time they appear.\n",
    "    truncation=True, #input text is truncated if it exceeds the model’s maximum context length\n",
    "    pad_token_id=tokenizer.eos_token_id, #padding token ID to use for sequences shorter than the model’s context length.\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face model ready\n",
      "RAG chain ready\n"
     ]
    }
   ],
   "source": [
    "#Build prompt and RAG chain\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_gen)\n",
    "print(\"Hugging Face model ready\")\n",
    "\n",
    "# \n",
    "# templated prompt with two required variables: {context} and {question}\n",
    "# At runtime, LangChain will format this template by substituting the actual context text and the user’s question.\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer clearly and concisely.\n",
    "\"\"\")\n",
    "\n",
    "# Build RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda q: get_context_text(q), #Takes the incoming input q and calls your function get_context_text(q) to retrieve retrieved documents / context text\n",
    "        \"question\": RunnablePassthrough(), #original input unchanged and stores it under the\n",
    "    }\n",
    "    | prompt #prompt expects a dict with keys matching the template variables (context, question) and returns a formatted prompt\n",
    "    | llm #Sends the formatted prompt to your Hugging Face model via the LangChain LLM wrapper.\n",
    "    | StrOutputParser() #final output is a clean string\n",
    ")\n",
    "print(\"RAG chain ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## **Question:** What is a Large Language Model?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### **Top Retrieved Chunks:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style='background-color:#FFF6A4; padding:8px; margin:6px 0;'><b>Chunk 1</b>: than just LLMs in isolation. That, however, brings us to the question: what are large language models? To begin answering this question in this chapter, let’s first explore the history of Language AI. A Recent History of Language AI The history of Language AI encompasses many developments and models aiming to represent and generate language, as illustrated in Figure 1-1. Figure 1-1. A peek into th...</div>\n",
       "<div style='background-color:#FFF6A4; padding:8px; margin:6px 0;'><b>Chunk 2</b>: models. What name we give one model or the other does not change how it behaves. Since the definition of the term “large language model” tends to evolve with the release of new models, we want to be explicit in what it means for this book. “Large” is arbitrary and what might be considered a large model today could be small tomorrow. There are currently many names for the same thing and to us, “lar...</div>\n",
       "<div style='background-color:#FFF6A4; padding:8px; margin:6px 0;'><b>Chunk 3</b>: referred to as large language models. Especially if they are considered to be “large.” In practice, this seems like a rather constrained description! What if we create a model with the same capabilities as GPT-3 but 10 times smaller? Would such a model fall outside the “large” language model categorization? Similarly, what if we released a model as big as GPT-4 that can perform accurate text class...</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  **Prompt Sent to Model:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```text\n",
       "Human: \n",
       "Use the following context to answer the user's question.\n",
       "\n",
       "Context:\n",
       "than just LLMs in isolation. That, however, brings us to the question: what\n",
       "are large language models? To begin answering this question in this chapter,\n",
       "let’s first explore the history of Language AI.\n",
       "A Recent History of Language AI\n",
       "The history of Language AI encompasses many developments and models\n",
       "aiming to represent and generate language, as illustrated in Figure 1-1.\n",
       "Figure 1-1. A peek into the history of Language AI.\n",
       "Language, however, is a tricky concept for computers. Text is unstructured\n",
       "in nature and loses its meaning when represented by zeros and ones\n",
       "(individual characters). As a result, throughout the history of Language AI,\n",
       "\n",
       "models. What name we give one model or the other does not change how it\n",
       "behaves\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### **Model Answer:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Human: \n",
       "Use the following context to answer the user's question.\n",
       "\n",
       "Context:\n",
       "than just LLMs in isolation. That, however, brings us to the question: what\n",
       "are large language models? To begin answering this question in this chapter,\n",
       "let’s first explore the history of Language AI.\n",
       "A Recent History of Language AI\n",
       "The history of Language AI encompasses many developments and models\n",
       "aiming to represent and generate language, as illustrated in Figure 1-1.\n",
       "Figure 1-1. A peek into the history of Language AI.\n",
       "Language, however, is a tricky concept for computers. Text is unstructured\n",
       "in nature and loses its meaning when represented by zeros and ones\n",
       "(individual characters). As a result, throughout the history of Language AI,\n",
       "\n",
       "models. What name we give one model or the other does not change how it\n",
       "behaves.\n",
       "Since the definition of the term “large language model” tends to evolve with\n",
       "the release of new models, we want to be explicit in what it means for this\n",
       "book. “Large” is arbitrary and what might be considered a large model\n",
       "today could be small tomorrow. There are currently many names for the\n",
       "same thing and to us, “large language models” are also models that do not\n",
       "generate text and can be run on consumer hardware.\n",
       "As such, aside from covering generative models, this book will also cover\n",
       "models with fewer than 1 billion parameters that do not generate text. We\n",
       "will explore how other models, such as embedding models, representation\n",
       "models, and even bag-of-words can be used to empower LLMs.\n",
       "\n",
       "referred to as large language models. Especially if they are considered to be\n",
       "“large.” In practice, this seems like a rather constrained description!\n",
       "What if we create a model with the same capabilities as GPT-3 but 10 times\n",
       "smaller? Would such a model fall outside the “large” language model\n",
       "categorization?\n",
       "Similarly, what if we released a model as big as GPT-4 that can perform\n",
       "accurate text classification but does not have any generative capabilities?\n",
       "Would it still qualify as a large “language model” if its primary function is\n",
       "not language generation, even though it still represents text?\n",
       "The problem with these kinds of definitions is that we exclude capable\n",
       "models. What name we give one model or the other does not change how it\n",
       "behaves.\n",
       "\n",
       "Question:\n",
       "What is a Large Language Model?\n",
       "\n",
       "Answer clearly and concisely.\n",
       "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# =========================================================\n",
    "#  Visualization utilities\n",
    "# =========================================================\n",
    "def highlight_chunks(chunks, color=\"yellow\"):\n",
    "    \"\"\"Display retrieved chunks with color highlights in Markdown.\"\"\"\n",
    "    highlights = []\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        text = c.page_content if hasattr(c, \"page_content\") else str(c)\n",
    "        # Clean up whitespace and tabs\n",
    "        text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "        highlights.append(f\"<div style='background-color:{color}; padding:8px; margin:6px 0;'>\"\n",
    "                          f\"<b>Chunk {i}</b>: {text[:400]}...</div>\")\n",
    "    return \"\\n\".join(highlights)\n",
    "\n",
    "def visualize_retrieval(question):\n",
    "    \"\"\"Show top chunks, prompt, and final answer side-by-side.\"\"\"\n",
    "    docs = retriever.invoke(question)\n",
    "    context_text = get_context_text(question)\n",
    "\n",
    "    display(Markdown(f\"## **Question:** {question}\"))\n",
    "    display(Markdown(\"### **Top Retrieved Chunks:**\"))\n",
    "    display(Markdown(highlight_chunks(docs, color=\"#FFF6A4\")))\n",
    "\n",
    "    # Show prompt content (truncated)\n",
    "    filled_prompt = prompt.invoke({\"context\": context_text, \"question\": question})\n",
    "    display(Markdown(\"###  **Prompt Sent to Model:**\"))\n",
    "    display(Markdown(f\"```text\\n{filled_prompt.to_string()[:800]}\\n```\"))\n",
    "\n",
    "    # Generate answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    display(Markdown(\"### **Model Answer:**\"))\n",
    "    display(Markdown(f\"> {answer}\"))\n",
    "\n",
    "# =========================================================\n",
    "# Run the demo\n",
    "# =========================================================\n",
    "visualize_retrieval(\"What is a Large Language Model?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
