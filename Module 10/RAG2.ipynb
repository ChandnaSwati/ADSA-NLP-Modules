{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "# =========================================================\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Documents into Chunks\n",
    "\n",
    "1. When we read an entire PDF, we often get very large blocks of text — sometimes thousands of words per page. But LLMs (and embedding models) have context limits — they can’t process all that at once.\n",
    "\n",
    "2. That’s why we split long documents into smaller, overlapping pieces, called chunks.\n",
    "\n",
    "3. Each chunk will later become:\n",
    "   - an embedding (vector representation)\n",
    "   - a retrieval unit (the part that gets fetched when the user asks a question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 598 pages\n",
      "Created 1134 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(\"/Users/swatichandna/SynologyDrive/GitHub/NLP/Module 10/ToyData/test.pdf\")\n",
    "raw_docs = loader.load()\n",
    "print(f\"Loaded {len(raw_docs)} pages\")\n",
    "\n",
    "\n",
    "\n",
    "# Chunk documents\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=120,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "print(f\"Created {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings for Each Chunk\n",
    "\n",
    "Each chunk of text  previous step must be converted into a vector — a list of numbers that captures its meaning.\n",
    "This process is called embedding.\n",
    "\n",
    "Why?\n",
    "1. Machines can’t understand text, but they can compare numbers.\n",
    "2. Each vector encodes the semantic meaning of a chunk.\n",
    "3. Similar meanings → vectors are close together in multi-dimensional space.\n",
    "4. So when a user asks a question, the query is also embedded, and we find the chunks whose vectors are most similar to it (via cosine similarity or L2 distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create embeddings + FAISS retriever\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"FAISS retriever ready\")\n",
    "\n",
    "\n",
    "\n",
    "# Helper to extract clean text\n",
    "def get_context_text(query):\n",
    "    results = retriever.invoke(query)\n",
    "    if not results:\n",
    "        return \"No relevant context found.\"\n",
    "    if hasattr(results[0], \"page_content\"):\n",
    "        texts = [doc.page_content for doc in results]\n",
    "    else:\n",
    "        texts = [str(doc) for doc in results]\n",
    "    return \"\\n\\n\".join(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hugging Face LLM setup\n",
    "model_id = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n",
    "\n",
    "hf_gen = pipeline(\n",
    "    task=\"text-generation\", #goal is to predict or generate text continuations\n",
    "    model=model,  #Passes in your already loaded language model\n",
    "    tokenizer=tokenizer,  #The tokenizer converts between text and token IDs\n",
    "    max_new_tokens=200, #maximum number of tokens the model is allowed to generate\n",
    "    temperature=0.3, #Controls the randomness of predictions,Lower = more deterministic, Higher = more creative/random.\n",
    "    do_sample=False, #sample probabilistically or pick the top-scoring tokens deterministically.False means greedy decoding — always choose the most likely next token (no randomness).  \n",
    "    top_p=0.9,  #nucleus sampling (a method to pick from only the most probable subset of tokens that together make up 90% of the probability mass).\n",
    "    top_k=50, #Used for top-k sampling, which restricts the model to choose from the top 50 most likely next tokens.\n",
    "    repetition_penalty=1.1, #Penalizes repeating tokens by slightly reducing their probability each time they appear.\n",
    "    truncation=True, #input text is truncated if it exceeds the model’s maximum context length\n",
    "    pad_token_id=tokenizer.eos_token_id, #padding token ID to use for sequences shorter than the model’s context length.\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build prompt and RAG chain\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_gen)\n",
    "print(\"Hugging Face model ready\")\n",
    "\n",
    "# \n",
    "# templated prompt with two required variables: {context} and {question}\n",
    "# At runtime, LangChain will format this template by substituting the actual context text and the user’s question.\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer clearly and concisely.\n",
    "\"\"\")\n",
    "\n",
    "# Build RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda q: get_context_text(q), #Takes the incoming input q and calls your function get_context_text(q) to retrieve retrieved documents / context text\n",
    "        \"question\": RunnablePassthrough(), #original input unchanged and stores it under the\n",
    "    }\n",
    "    | prompt #prompt expects a dict with keys matching the template variables (context, question) and returns a formatted prompt\n",
    "    | llm #Sends the formatted prompt to your Hugging Face model via the LangChain LLM wrapper.\n",
    "    | StrOutputParser() #final output is a clean string\n",
    ")\n",
    "print(\"RAG chain ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "#  Visualization utilities\n",
    "# =========================================================\n",
    "def highlight_chunks(chunks, color=\"yellow\"):\n",
    "    \"\"\"Display retrieved chunks with color highlights in Markdown.\"\"\"\n",
    "    highlights = []\n",
    "    for i, c in enumerate(chunks, 1):\n",
    "        text = c.page_content if hasattr(c, \"page_content\") else str(c)\n",
    "        # Clean up whitespace and tabs\n",
    "        text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "        highlights.append(f\"<div style='background-color:{color}; padding:8px; margin:6px 0;'>\"\n",
    "                          f\"<b>Chunk {i}</b>: {text[:400]}...</div>\")\n",
    "    return \"\\n\".join(highlights)\n",
    "\n",
    "def visualize_retrieval(question):\n",
    "    \"\"\"Show top chunks, prompt, and final answer side-by-side.\"\"\"\n",
    "    docs = retriever.invoke(question)\n",
    "    context_text = get_context_text(question)\n",
    "\n",
    "    display(Markdown(f\"## **Question:** {question}\"))\n",
    "    display(Markdown(\"### **Top Retrieved Chunks:**\"))\n",
    "    display(Markdown(highlight_chunks(docs, color=\"#FFF6A4\")))\n",
    "\n",
    "    # Show prompt content (truncated)\n",
    "    filled_prompt = prompt.invoke({\"context\": context_text, \"question\": question})\n",
    "    display(Markdown(\"###  **Prompt Sent to Model:**\"))\n",
    "    display(Markdown(f\"```text\\n{filled_prompt.to_string()[:800]}\\n```\"))\n",
    "\n",
    "    # Generate answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    display(Markdown(\"### **Model Answer:**\"))\n",
    "    display(Markdown(f\"> {answer}\"))\n",
    "\n",
    "# =========================================================\n",
    "# Run the demo\n",
    "# =========================================================\n",
    "visualize_retrieval(\"What is a Large Language Model?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
