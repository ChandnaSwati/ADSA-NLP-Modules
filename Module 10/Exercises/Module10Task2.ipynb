{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUD5ZW-iJQxA",
        "outputId": "603ced4b-20c5-4c95-aab2-c5b3dcd8c661"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Module 10: Retrieval-Augmented Generation & Vector Search\n",
        "# Lab 2 â€“ Baseline LLM QA without RAG\n",
        "# ============================================\n",
        "# Author: Dr. Dasha Trofimova\n",
        "# Course: M.Sc. Applied Data Science & AI\n",
        "# --------------------------------------------\n",
        "# Learning Goals:\n",
        "# - Recognize the limitations of relying solely on LLM parametric memory\n",
        "# - Compare factual accuracy with and without retrieval grounding\n",
        "# - Experiment with prompt engineering to improve standalone reasoning\n",
        "# --------------------------------------------\n",
        "# Lab Objectives:\n",
        "# 1. Use TinyLlama or another open-source LLM for direct QA\n",
        "# 2. Ask factual and numerical questions to observe hallucinations\n",
        "# 3. Compare responses against RAG-generated answers\n",
        "# 4. Reflect on how retrieval grounding improves reliability\n",
        "# ============================================\n",
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMkzDnSIJV1k",
        "outputId": "f127ea35-24e1-4076-c84b-a78696537fb4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Pick your model\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(\"cuda available?\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"device:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"running on CPU (this will be slower)\")\n",
        "\n",
        "# Create the generation pipeline\n",
        "llm_pipeline = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=MODEL_NAME,\n",
        "    tokenizer=MODEL_NAME,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",      # GPU if available, CPU otherwise\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "coZv9Tu2JkUR"
      },
      "outputs": [],
      "source": [
        "MAX_NEW_TOKENS = 80   # limit how long answers can be\n",
        "TEMPERATURE = 0.2     # a little sampling, but still controlled\n",
        "\n",
        "def build_prompt(user_msg: str) -> str:\n",
        "    \"\"\"\n",
        "    Build a clean prompt for TinyLlama.\n",
        "    We keep the format simple and predictable.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You are a helpful, concise AI assistant.\\n\"\n",
        "        \"Answer the user clearly. Do not invent a new 'User:' turn.\\n\\n\"\n",
        "        f\"User: {user_msg}\\n\"\n",
        "        \"Assistant:\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def run_model(user_msg: str) -> str:\n",
        "    \"\"\"\n",
        "    Send a single user message to the model and return only the assistant's answer.\n",
        "    We also trim off extra self-chat the model might generate.\n",
        "    \"\"\"\n",
        "    prompt = build_prompt(user_msg)\n",
        "\n",
        "    result = llm_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        temperature=TEMPERATURE,\n",
        "        do_sample=True,   # small models need some sampling to not freeze\n",
        "        pad_token_id=llm_pipeline.tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    full_text = result[0][\"generated_text\"]\n",
        "\n",
        "    # We expect something like:\n",
        "    # \"You are a helpful... User: <question>\\nAssistant: <answer> ... maybe more\"\n",
        "    # We'll pull just the assistant part.\n",
        "    if \"Assistant:\" in full_text:\n",
        "        answer_only = full_text.split(\"Assistant:\", 1)[-1].strip()\n",
        "    else:\n",
        "        answer_only = full_text.strip()\n",
        "\n",
        "    # Sometimes tiny models keep going and hallucinate a new \"User:\".\n",
        "    if \"User:\" in answer_only:\n",
        "        answer_only = answer_only.split(\"User:\", 1)[0].strip()\n",
        "\n",
        "    return answer_only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX_v4MNeOZl-",
        "outputId": "7bc50e7f-6b2a-4b39-b2f7-fde5dfcf8320"
      },
      "outputs": [],
      "source": [
        "def chat_loop():\n",
        "    print(\"\\n=== TinyLlama Chat ===\")\n",
        "    print(\"Ask anything. Type 'exit' to stop.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_q = input(\"You: \")\n",
        "        if user_q.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Bye ðŸ‘‹\")\n",
        "            break\n",
        "\n",
        "        answer = run_model(user_q)\n",
        "        print(\"Assistant:\", answer, \"\\n\")\n",
        "\n",
        "chat_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMOxMqfzOdUl"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
