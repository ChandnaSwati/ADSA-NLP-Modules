{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIY4VKamDZ_5",
        "outputId": "6511bc67-da73-4d68-abfe-29e6faac0d85"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Module 10: Retrieval-Augmented Generation & Vector Search\n",
        "# Lab 1 – Build a RAG Pipeline with Multiple PDFs\n",
        "# ============================================\n",
        "# Author: Dr. Dasha Trofimova\n",
        "# Course: M.Sc. Applied Data Science & AI\n",
        "# --------------------------------------------\n",
        "# Learning Goals:\n",
        "# - Understand the architecture of Retrieval-Augmented Generation (RAG)\n",
        "# - Load and preprocess multiple PDFs into text chunks\n",
        "# - Create embeddings and store them in a vector database (Chroma)\n",
        "# - Retrieve contextually relevant chunks for question answering\n",
        "# --------------------------------------------\n",
        "# Lab Objectives:\n",
        "# 1. Load PDFs using PyPDFLoader and chunk text\n",
        "# 2. Generate embeddings with OpenAI or Hugging Face models\n",
        "# 3. Store vectors persistently in Chroma\n",
        "# 4. Build a retriever and connect it to a local LLM (TinyLlama)\n",
        "# 5. Run interactive QA grounded in the PDF content\n",
        "# ============================================\n",
        "!pip install langchain langchain-community langchain-chroma langchain-huggingface \\\n",
        "    transformers accelerate bitsandbytes sentence-transformers PyPDF2 python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_-VJK5TD9UM",
        "outputId": "eea9e07d-8da4-4ad1-8878-71f6da92a0bc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "print(\"Imports loaded.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXh1l3fXEETF",
        "outputId": "f8d6ee68-baa9-4965-9058-abcf188d358d"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "hf_gen_pipeline = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model_name,\n",
        "    tokenizer=model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# Wrap the transformers pipeline so it looks like an LLM to LangChain\n",
        "llm = HuggingFacePipeline(pipeline=hf_gen_pipeline)\n",
        "\n",
        "print(f\"LLM ready: {model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VDENMLNEIs0",
        "outputId": "882aac1b-a576-4412-f08a-79504922a5a1"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "print(\"Embedding model ready (MiniLM).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "nqRYhQTeEJgY",
        "outputId": "a765f995-c402-4d70-ec7c-0a12d1b90671"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf\n",
        "\n",
        "def load_pdfs_colab():\n",
        "    \"\"\"\n",
        "    Lets you upload one or more PDF files in Colab.\n",
        "    Returns a list of LangChain Document objects (each page is one Document).\n",
        "    \"\"\"\n",
        "    from langchain_community.document_loaders import PyPDFLoader  # keep import here so reload works after installs\n",
        "    from google.colab import files\n",
        "\n",
        "    print(\"Please upload one or more PDF files.\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    all_pages = []\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        if not filename.lower().endswith(\".pdf\"):\n",
        "            print(f\"Skipping non-PDF file: {filename}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Loading {filename} ...\")\n",
        "        loader = PyPDFLoader(filename)\n",
        "        try:\n",
        "            pages = loader.load()\n",
        "            print(f\"  -> Loaded {len(pages)} pages.\")\n",
        "            all_pages.extend(pages)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "    print(f\"\\nTotal pages loaded across all PDFs: {len(all_pages)}\")\n",
        "    return all_pages\n",
        "\n",
        "\n",
        "pages = load_pdfs_colab()\n",
        "\n",
        "if len(pages) == 0:\n",
        "    raise ValueError(\"No PDF pages loaded. Please rerun cell and upload at least one PDF.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_rObRC3EObQ",
        "outputId": "c56f830f-bfc2-4487-b59e-f56c3162186b"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,   # tune for context window\n",
        "    chunk_overlap=200  # tune for continuity\n",
        ")\n",
        "\n",
        "pages_split = text_splitter.split_documents(pages)\n",
        "print(f\"Created {len(pages_split)} chunks.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-z8-LB6ERFy",
        "outputId": "47af39ca-0f20-43db-8705-3b632f834e4b"
      },
      "outputs": [],
      "source": [
        "persist_directory = \"./chroma_store\"\n",
        "collection_name = \"rag_docs\"\n",
        "\n",
        "# make sure directory exists\n",
        "os.makedirs(persist_directory, exist_ok=True)\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=pages_split,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=persist_directory,\n",
        "    collection_name=collection_name\n",
        ")\n",
        "\n",
        "print(\"✅ Chroma vector DB is ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YhdCyygsEUcq"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 2}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "c8LF64bSEVsZ"
      },
      "outputs": [],
      "source": [
        "def retrieve_context(query: str):\n",
        "    \"\"\"\n",
        "    Return top-k relevant LangChain Documents for a user query.\n",
        "    \"\"\"\n",
        "    docs = retriever.invoke(query)\n",
        "    return docs\n",
        "\n",
        "def build_prompt(user_question: str, docs):\n",
        "    # Build a trimmed context from top-k docs\n",
        "    context_blocks = []\n",
        "    for i, d in enumerate(docs):\n",
        "        src = d.metadata.get(\"source\", \"unknown\")\n",
        "        page = d.metadata.get(\"page\", \"N/A\")\n",
        "        context_blocks.append(\n",
        "            f\"[Chunk {i+1} | source: {src} | page {page}]\\n{d.page_content}\"\n",
        "        )\n",
        "\n",
        "    context_text = \"\\n\\n---\\n\\n\".join(context_blocks)\n",
        "\n",
        "    prompt = (\n",
        "        \"You are an assistant. You should primarily answer using the provided context.\\n\"\n",
        "        \"If the question is basic general knowledge or simple math, you MAY answer it directly.\\n\"\n",
        "        \"Otherwise, if the answer is not in the context, say:\\n\"\n",
        "        \"\\\"I don't see that in the provided documents.\\\"\\n\"\n",
        "        \"When you use the documents, cite chunks like [Chunk 2].\\n\\n\"\n",
        "        \"CONTEXT:\\n\"\n",
        "        f\"{context_text}\\n\\n\"\n",
        "        \"QUESTION:\\n\"\n",
        "        f\"{user_question}\\n\\n\"\n",
        "        \"FINAL ANSWER:\\n\"\n",
        "    )\n",
        "\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuOmnVasEZDK",
        "outputId": "fdf783f9-06e4-4eb1-c82c-79b15bb5a4dc"
      },
      "outputs": [],
      "source": [
        "def rag_qa_loop():\n",
        "    print(\"\\n=== RAG QA (multi-PDF, open source model, no agents) ===\")\n",
        "    print(\"Type your question. Type 'exit' to stop.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nYour question: \")\n",
        "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Goodbye.\")\n",
        "            break\n",
        "\n",
        "        # 1. retrieve top-k chunks\n",
        "        docs = retrieve_context(user_query)\n",
        "        if not docs:\n",
        "            print(\"\\nNo relevant context found in your PDFs.\")\n",
        "            continue\n",
        "\n",
        "        # 2. build prompt for the LLM\n",
        "        messages = build_prompt(user_query, docs)\n",
        "\n",
        "        # 3. run the model\n",
        "        answer = llm.invoke(messages)\n",
        "\n",
        "        # 4. display\n",
        "        print(\"\\n=== ANSWER ===\")\n",
        "        print(answer)\n",
        "\n",
        "rag_qa_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG-y5JPaIXdQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
