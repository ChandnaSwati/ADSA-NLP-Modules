{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect & Prepare Your Own Documents\n",
    "\n",
    "Task:\n",
    "\n",
    "1. Create a folder named my_docs/.\n",
    "2. Place at least 3 text-based files in it — .txt, .md, or extracted .pdf text.\n",
    "  \n",
    "Examples:\n",
    "  - A research paper summary you wrote\n",
    "  - A blog article about AI or sustainability\n",
    "  - Course notes or company reports\n",
    "\n",
    "3. Load them into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = \"my_docs\"\n",
    "documents = []\n",
    "\n",
    "# TODO: Load your own files\n",
    "# Verify their data loaded properly.\n",
    "# Write a few lines describing what kind of documents they chose and why\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith((\".txt\", \".md\")):\n",
    "        with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(documents[0][:300])  # preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Your Texts\n",
    "\n",
    "**Goal**: Break long text into manageable pieces for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=200):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# TODO: Apply chunking to all your documents\n",
    "# Try different chunk sizes (100, 200, 400).\n",
    "# Which size produced more relevant retrievals later? Why?\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "\n",
    "print(\" Total chunks created:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your Own Retriever (Semantic)\n",
    "\n",
    "**Goal**: Retrieve the most relevant chunks using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# TODO: Embed your chunks\n",
    "chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)\n",
    "\n",
    "def retrieve_chunks(query, k=2):\n",
    "    query_embed = embedder.encode(query, convert_to_tensor=True)\n",
    "    scores = util.cos_sim(query_embed, chunk_embeddings)[0]\n",
    "    top_k = torch.topk(scores, k)\n",
    "    return [chunks[i] for i in top_k.indices]\n",
    "\n",
    "# TEST\n",
    "print(retrieve_chunks(\"What is discussed about AI ethics?\", k=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Retrieval with Generation\n",
    "\n",
    "**Goal**: Use retrieved chunks as context for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\", max_new_tokens=120, temperature=0.3)\n",
    "\n",
    "def mini_rag(query, k=2):\n",
    "    retrieved = retrieve_chunks(query, k)\n",
    "    context = \"\\n\".join(retrieved)\n",
    "    prompt = f\"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    return generator(prompt)[0]['generated_text']\n",
    "\n",
    "# TODO: Try 2–3 questions based on your own documents\n",
    "#Observe how the generator uses their context.\n",
    "# Run the same question with and without retrieval.\n",
    "# Compare and describe differences.\n",
    "print(mini_rag(\"Summarize the main idea from my document.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a LangChain RAG Chain\n",
    "\n",
    "**Goal**: Chain the retrieval and generation steps modularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer clearly and concisely:\n",
    "\"\"\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": lambda q: \"\\n\".join(retrieve_chunks(q, 2)), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# TODO: Run your chain\n",
    "# Modify the retriever to return top 3 instead of 2.\n",
    "# Print retrieved chunks before answering.\n",
    "# Discuss if adding more chunks improved or worsened quality.\n",
    "print(rag_chain.invoke(\"What are the key challenges mentioned in my research?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Reflection\n",
    "\n",
    "### Goal\n",
    "Critically assess and document the performance of **your own RAG system**.  \n",
    "Reflect on how your retrieval and generation pipeline behaved with your chosen documents.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Questions\n",
    "\n",
    "| **Question** | **Students Write** |\n",
    "|---------------|--------------------|\n",
    "| **How many documents did you use?** |  |\n",
    "| **What type of content (topic/domain)?** |  |\n",
    "| **Which retrieval size (chunk length, `top_k`) worked best?** |  |\n",
    "| **Did the model produce hallucinations? When?** |  |\n",
    "| **What improvement would you try next?** |  |\n",
    "\n",
    "---\n",
    "\n",
    "### Optional Extension Ideas\n",
    "\n",
    "- **Try another model**, e.g. `google/flan-t5-base` or `facebook/bart-large`, and compare outputs.  \n",
    "- **Visualize cosine similarity scores** between your query and retrieved chunks as a bar chart to better understand retrieval ranking.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
