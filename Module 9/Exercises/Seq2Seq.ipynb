{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare a Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 1. CREATE A SMALL DATASET\n",
    "# =============================================\n",
    "pairs = [\n",
    "    (\"i love ai\", \"ai love i\"),\n",
    "    (\"students learn models\", \"models learn students\"),\n",
    "    (\"deep learning rocks\", \"rocks learning deep\"),\n",
    "    (\"attention helps learning\", \"learning helps attention\"),\n",
    "    (\"transformers are powerful\", \"powerful are transformers\"),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build Tokenizer & Vocabulary\n",
    "\n",
    "Tokenize the text and build mappings (word → index, index → word).\n",
    "(Hint: you can use Python’s split() for simplicity.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Encoder–Decoder Model (using LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skeleton code\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (h, c) = self.lstm(embedded)\n",
    "        return h, c\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(embedded, (h, c))\n",
    "        predictions = self.fc(output)\n",
    "        return predictions, h, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Combine Encoder & Decoder\n",
    "\n",
    "During training:\n",
    "1. Encoder reads the input sequence.\n",
    "2. Decoder generates tokens step-by-step, using the previous token as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the Model\n",
    "\n",
    "1. Use CrossEntropyLoss.\n",
    "2. Use teacher forcing (feeding the true token sometimes instead of predicted).\n",
    "3. Train for ~300 epochs.\n",
    "4. Print sample predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate on new sentences \n",
    "\n",
    "- \"i study ai\" → ?\n",
    "- \"transformers change nlp\" → ?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
