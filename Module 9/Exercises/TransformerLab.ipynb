{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 — Tokenization & Vocabulary\n",
    "- Create a simple tokenizer and vocabulary.  #\n",
    "- Convert a sentence into integer token IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blanks\n",
    "\n",
    "PAD, UNK, CLS = \"<pad>\", \"<unk>\", \"<cls>\"\n",
    "sentence = \"I love artificial intelligence\"\n",
    "\n",
    "# Step 1: Split and lowercase\n",
    "tokens = [CLS] + sentence.________().split()\n",
    "\n",
    "# Step 2: Build a small vocabulary\n",
    "vocab = sorted(set(tokens + [PAD, UNK]))\n",
    "stoi = {w: i for i, w in enumerate(________)}\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "\n",
    "# Step 3: Encode the sentence\n",
    "ids = [stoi.get(tok, stoi[________]) for tok in tokens]\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Reflect:**  \n",
    "- Why do we add a `<cls>` token?  \n",
    "- How would you handle unknown words in new sentences?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: \n",
    "Create sinusoidal positional encodings to inject order information into token embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "\n",
    "# Fill in the blanks\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / _______))\n",
    "    pe[:, 0::2] = torch.sin(________ * div_term)\n",
    "    pe[:, 1::2] = torch.cos(________ * div_term)\n",
    "    return pe\n",
    "\n",
    "pe = positional_encoding(10, 8)\n",
    "print(pe[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Reflect:**  \n",
    "- Why do we alternate sine and cosine values?  \n",
    "- What benefit do sinusoidal encodings have compared to learned positional embeddings?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "- Implement the attention mechanism manually:  \n",
    "- Softmax(QKᵀ / √dₖ) × V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Fill in the blanks\n",
    "\n",
    "Q = torch.randn(1, 3, 4)   # [batch, tokens, d_model]\n",
    "K = torch.randn(1, 3, 4)\n",
    "V = torch.randn(1, 3, 4)\n",
    "\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(______)\n",
    "attn = torch.softmax(scores, dim=-1)\n",
    "out = torch.matmul(attn, ________)\n",
    "\n",
    "print(\"Attention weights:\\n\", attn)\n",
    "print(\"Output:\\n\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Reflect:**  \n",
    "- What would happen if we didn’t scale by √dₖ?  \n",
    "- Which rows in the attention matrix correspond to which tokens?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "Simulate 2 attention heads by splitting Q, K, and V manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blanks\n",
    "\n",
    "import torch, math\n",
    "torch.manual_seed(0)\n",
    "\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "d_head = d_model // num_heads\n",
    "\n",
    "x = torch.randn(1, 3, d_model)\n",
    "\n",
    "# Create projection matrices\n",
    "W_q = torch.randn(d_model, d_model)\n",
    "W_k = torch.randn(d_model, d_model)\n",
    "W_v = torch.randn(d_model, d_model)\n",
    "\n",
    "Q, K, V = x @ W_q, x @ W_k, x @ W_v\n",
    "\n",
    "# Split into heads\n",
    "Q = Q.view(1, 3, num_heads, d_head).transpose(1, 2)\n",
    "K = K.view(1, 3, num_heads, d_head).transpose(1, 2)\n",
    "V = V.view(1, 3, num_heads, d_head).transpose(1, 2)\n",
    "\n",
    "# Compute attention per head\n",
    "attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(________)\n",
    "attn = torch.softmax(attn_scores, dim=-1)\n",
    "out = torch.matmul(attn, V)\n",
    "\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Reflect:**  \n",
    "- Why do we use multiple heads instead of one?  \n",
    "- What different relationships could each head learn?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "Implement a minimal Transformer encoder block using:  \n",
    "1️. Multi-Head Attention  \n",
    "2️. Feed-Forward Network  \n",
    "3️. Residual + Layer Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Fill in the blanks\n",
    "\n",
    "class MiniEncoder(nn.Module):\n",
    "    def __init__(self, d_model=8, d_ff=16):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=2, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.mha(x, x, x)\n",
    "        x = self.ln1(x + ________)\n",
    "        ff_out = self.ff(x)\n",
    "        return self.ln2(x + ________)\n",
    "\n",
    "# Test block\n",
    "x = torch.randn(1, 4, 8)\n",
    "enc = MiniEncoder()\n",
    "y = enc(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflect:**  \n",
    "1. Why do we add residual connections?  \n",
    "2. What happens if LayerNorm is removed from the network?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
