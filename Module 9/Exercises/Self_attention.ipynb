{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Module 8: From Classical to Deep NLP\n",
    "# LSTM Critical Thinking & Application Lab\n",
    "# ============================================\n",
    "# Author: Prof. Dr. Swati Chandna\n",
    "# Course: M.Sc. Applied Data Science & AI\n",
    "# --------------------------------------------\n",
    "# Learning Goals:\n",
    "# - Explain how attention distributes “focus” across words.\n",
    "# - Connect the dot-product formula to linguistic intuition.\n",
    "# - Implement and visualize scaled dot-product attention.\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — Predict the Attention Weights\n",
    "\n",
    "Before coding, let’s reason like the model!\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Choose one or more of the following sentences:\n",
    "   - **I love AI**\n",
    "   - **AI loves me**\n",
    "   - **Birds can fly**\n",
    "   - **Students learn quickly**\n",
    "\n",
    "2. For each sentence, fill in a small **attention weight table** (on code).  \n",
    "   Each **row** corresponds to a *query word*, and each **column** to a *key word*.  \n",
    "   The values in every row must sum to **1.0**.\n",
    "\n",
    "   Example:\n",
    "\n",
    "   | From → To | I | love | AI |\n",
    "   |------------|---|------|----|\n",
    "   | **I** | 0.25 | 0.35 | 0.40 |\n",
    "   | **love** | 0.30 | 0.30 | 0.40 |\n",
    "   | **AI** | 0.20 | 0.40 | 0.40 |\n",
    "\n",
    "\n",
    "\n",
    "3. Discuss with your partner:\n",
    "   - Which word does each word depend on most — and why?  \n",
    "   - Would the pattern change if the sentence were *“AI loves me”*?  \n",
    "   - Is attention symmetric (if *love → AI* = 0.7, does *AI → love* = 0.7)?\n",
    "\n",
    "**Hint:** Think about *subject*, *verb*, and *object* roles.  \n",
    "The verb often links both sides, so it typically shares its attention between the subject and the object.\n",
    "\n",
    "---\n",
    "\n",
    "##  Part 2 — Compute Scaled Dot-Product Attention\n",
    "\n",
    "Now we’ll compute the same idea mathematically.\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "### Your Tasks\n",
    "\n",
    "1. **Create small embeddings** for each word in your chosen sentence (e.g., 3–5 dimensions).  \n",
    "\n",
    "2. **Initialize random weight matrices** for queries, keys, and values:\n",
    "   - \\( W_Q \\), \\( W_K \\), \\( W_V \\)\n",
    "\n",
    "3. **Compute:**\n",
    "   - \\( Q = X W_Q \\)\n",
    "   - \\( K = X W_K \\)\n",
    "   - \\( V = X W_V \\)\n",
    "\n",
    "4. **Calculate attention scores:**\n",
    "   $$\n",
    "   \\text{scores} = \\frac{Q K^{T}}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "5. **Apply softmax** to obtain attention weights:  \n",
    "    $$\n",
    "   \\text{weights} = \\text{softmax}(\\text{scores})\n",
    "   $$\n",
    "\n",
    "6. **Compute final outputs:**  \n",
    "   $$\n",
    "   \\text{output} = \\text{weights} \\times V\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Visualize Attention\n",
    "\n",
    "Create two visualizations for your sentence:\n",
    "\n",
    "- **Heatmap:** shows how much each word attends to others.  \n",
    "- **Arrows:** show direction and strength of attention between words.\n",
    "\n",
    "Example visual goals:\n",
    "- Darker cells = stronger attention.  \n",
    "- Thicker arrows = stronger connections.\n",
    "\n",
    "---\n",
    "\n",
    "### Reflection\n",
    "\n",
    "- Which word focuses most on others?  \n",
    "- Do verbs distribute attention differently than nouns?  \n",
    "- How does sentence order affect attention patterns?  \n",
    "- Would adding an adverb (*“I love AI deeply”*) shift the weights?\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
