{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Transformers?\n",
    "\n",
    "### **Background**\n",
    "Traditional models like **RNNs** or **LSTMs** read text *sequentially*, meaning the processing of one word depends on the previous one.  \n",
    "This causes two main issues:\n",
    "\n",
    "1. **Slow computation** – no parallelism  \n",
    "2. **Weak long-term dependencies** – difficult to remember distant words  \n",
    "\n",
    "---\n",
    "\n",
    "### **The Transformer Solution**\n",
    "Introduced by **Vaswani et al. (2017)** in the paper *“Attention is All You Need”*,  \n",
    "the **Transformer** solves these issues by allowing **each word to directly attend to every other word**.\n",
    "\n",
    "This enables:\n",
    "- **Parallel computation**\n",
    "- **Global context understanding**\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts**\n",
    "\n",
    "| Concept | Description |\n",
    "|----------|--------------|\n",
    "| **Self-Attention** | Allows each token to gather information from all other tokens in a sequence. |\n",
    "| **Multi-Head Attention** | Uses multiple attention “heads” to learn different types of relationships (e.g., syntax, semantics). |\n",
    "|  **Positional Encoding** | Injects information about word order, since self-attention itself has no sense of sequence. |\n",
    "|  **Feed-Forward Networks** | Non-linear layers applied to each position to refine learned features. |\n",
    "| **Residual Connections + LayerNorm** | Help stabilize training and ensure better gradient flow through deep networks. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Encoder vs Decoder**\n",
    "\n",
    "| Component | Function | Typical Use |\n",
    "|------------|-----------|--------------|\n",
    "| **Encoder** | Reads and encodes the input sequence into contextual representations. | Classification, embeddings, BERT-like models |\n",
    "| **Decoder** | Generates the output sequence step by step, attending to encoder outputs. | Translation, text generation, GPT-like models |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1️: Tokenization\n",
    "Sentence → [\"I\", \"love\", \"artificial\", \"intelligence\", \".\"]\n",
    "Each word is assigned an ID from the vocabulary.\n",
    "Example: `[101, 2000, 3562, 8007, 102]`\n",
    "\n",
    "\n",
    "# Step 2️: Embedding\n",
    "Each token is turned into a vector of numbers (say 4 dimensions for simplicity):\n",
    "| Token | Embedding (simplified) |\n",
    "|--------|------------------------|\n",
    "| I | [0.1, 0.3, 0.5, 0.7] |\n",
    "| love | [0.8, 0.2, 0.4, 0.6] |\n",
    "| artificial | [0.3, 0.9, 0.1, 0.5] |\n",
    "| intelligence | [0.4, 0.8, 0.9, 0.3] |\n",
    "| . | [0.2, 0.1, 0.2, 0.1] |\n",
    "\n",
    "\n",
    "# Step 3️: Positional Encoding\n",
    "Since attention has no order awareness, we add a position signal to each embedding.\n",
    "Example (added element-wise):\n",
    "| Position | Encoding | Resulting vector (Embedding + Position) |\n",
    "|-----------|-----------|---------------------------------------|\n",
    "| 1 | [0.0, 0.1, 0.2, 0.3] | [0.1, 0.4, 0.7, 1.0] |\n",
    "| 2 | [0.2, 0.3, 0.4, 0.5] | [1.0, 0.5, 0.8, 1.1] |\n",
    "| … | … | … |\n",
    "\n",
    "\n",
    "# Step 4️: Self-Attention (Intuition)\n",
    "Each token looks at **all others** to understand context.\n",
    "- “I” attends strongly to “love” → who is doing the action.\n",
    "- “love” attends to “artificial” and “intelligence” → what is being loved.\n",
    "- “artificial” attends to “intelligence” → adjective–noun relation.\n",
    "\n",
    "\n",
    "This is captured mathematically by the **attention weights** matrix:\n",
    "\n",
    "\n",
    "| Query → Key | I | love | artificial | intelligence | . |\n",
    "|--------------|---|------|-------------|---------------|---|\n",
    "| **I** | 0.10 | **0.70** | 0.15 | 0.05 | 0.00 |\n",
    "| **love** | 0.30 | 0.10 | **0.30** | **0.25** | 0.05 |\n",
    "| **artificial** | 0.05 | 0.10 | 0.20 | **0.60** | 0.05 |\n",
    "| **intelligence** | 0.05 | 0.05 | **0.30** | 0.55 | 0.05 |\n",
    "| **.** | 0.00 | 0.05 | 0.10 | 0.10 | **0.75** |\n",
    "\n",
    "\n",
    "Each row (Query) shows how much that word focuses on the others.\n",
    "\n",
    "\n",
    "# Step 5️: Weighted Sum (Context Vectors)\n",
    "After attention, each token gets a **contextual embedding** — a blend of other word meanings weighted by attention scores.\n",
    "Example (conceptually):\n",
    "- “I” → now knows about “love”.\n",
    "- “love” → enriched by “artificial” + “intelligence”.\n",
    "- “intelligence” → retains self + input from “artificial”.\n",
    "\n",
    "\n",
    "# Step 6️: Feed-Forward & Residual\n",
    "Each contextual embedding passes through a small feed-forward network and is added back to the original vector → stabilizes and refines meaning.\n",
    "\n",
    "\n",
    "# Step 7️: Multi-Head Attention\n",
    "Several heads repeat this process focusing on different relationships:\n",
    "- Head 1: Grammar (who–does–what)\n",
    "- Head 2: Semantics (what concepts connect)\n",
    "- Head 3: Position (which comes first)\n",
    "Combined → rich, multi-dimensional context.\n",
    "\n",
    "\n",
    "# Step 8️: Output Interpretation\n",
    "The model can now:\n",
    "- Encode “I love artificial intelligence.” into a **meaning-rich vector** (for classification).\n",
    "- Or pass it to a **decoder** (for translation/generation).\n",
    "\n",
    "\n",
    "**Key takeaway:**\n",
    "Transformers understand context by **attending to every word at once** —\n",
    "not by reading left-to-right, but by building a global map of relationships."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
