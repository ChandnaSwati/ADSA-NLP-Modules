{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embeddings\n",
    "\n",
    "Convert words into dense vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs:\n",
      "tensor([[1, 2, 3],\n",
      "        [1, 4, 0]])\n",
      "\n",
      " Embedding output shape: torch.Size([2, 3, 8])\n",
      "\n",
      " Embeddings for first sentence ('i love ai'):\n",
      " tensor([[ 0.4743,  2.3712,  0.1601, -0.7399,  0.3953, -2.1967, -0.2220, -0.7519],\n",
      "        [ 0.1516, -0.8838, -0.7086, -0.2069,  0.0796, -0.3800,  0.7540,  1.1430],\n",
      "        [ 0.3600, -0.4320, -0.3064, -0.6688, -0.5681, -0.1112, -0.4809, -0.9226]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Import the PyTorch library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1️. Define a small vocabulary\n",
    "# -------------------------------------------------------------\n",
    "# Each word is mapped to a unique integer ID.\n",
    "# <pad> is used for padding shorter sequences in a batch.\n",
    "vocab = {\n",
    "    \"<pad>\": 0,\n",
    "    \"i\": 1,\n",
    "    \"love\": 2,\n",
    "    \"ai\": 3,\n",
    "    \"bugs\": 4\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2️. Create example sentences\n",
    "# -------------------------------------------------------------\n",
    "# We'll encode two short sentences using the token IDs.\n",
    "# Each row in the tensor is one sentence.\n",
    "# Shape = (batch_size = 2, sequence_length = 3)\n",
    "x = torch.tensor([\n",
    "    [1, 2, 3],   # \"i love ai\"\n",
    "    [1, 4, 0]    # \"i bugs <pad>\"\n",
    "])\n",
    "\n",
    "print(\"Input token IDs:\")\n",
    "print(x)\n",
    "# Output:\n",
    "# tensor([[1, 2, 3],\n",
    "#         [1, 4, 0]])\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3️. Create an Embedding layer\n",
    "# -------------------------------------------------------------\n",
    "# The embedding layer stores a trainable vector for each word ID.\n",
    "# Here, we choose an embedding dimension of 8 → each word will\n",
    "# be represented as an 8-dimensional vector.\n",
    "emb = nn.Embedding(num_embeddings=len(vocab), embedding_dim=8)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4️. Pass our token IDs through the embedding layer\n",
    "# -------------------------------------------------------------\n",
    "# This replaces each token ID with its corresponding vector.\n",
    "out = emb(x)  # Shape = (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "print(\"\\n Embedding output shape:\", out.shape)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5️. Inspect the embeddings\n",
    "# -------------------------------------------------------------\n",
    "# For clarity, print embeddings of the first sentence\n",
    "print(\"\\n Embeddings for first sentence ('i love ai'):\\n\", out[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Questions \n",
    "\n",
    "1. Why does the embedding layer output continuous values instead of integers?\n",
    "2. How would increasing the embedding dimension (e.g., from 8 → 128) affect model capacity?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
